{"cells":[{"cell_type":"markdown","metadata":{"id":"JxAu7ozh5dCn"},"source":["#### 1. LeNet-5\n","\n","- 1995년 얀 르쿤이 합성곱 신경망 개념을 개발하면서 사용한 모델\n","\n","- 합성곱(convolutional)과 다운 샘플링(sub-sampling 혹은 polling)을 사용하여 사진 분류"]},{"cell_type":"markdown","metadata":{"id":"5fV7A3thOlJ4"},"source":["##### LeNet-5 architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3mIoUmW5dCp"},"outputs":[],"source":["# ----------------------------------------------------------------\n","#         Layer (type)               Output Shape         Param #\n","# ================================================================\n","#             Conv2d-1         [-1, 16, 220, 220]           1,216\n","#               ReLU-2         [-1, 16, 220, 220]               0\n","#          MaxPool2d-3         [-1, 16, 110, 110]               0\n","#             Conv2d-4         [-1, 32, 106, 106]          12,832\n","#               ReLU-5         [-1, 32, 106, 106]               0\n","#          MaxPool2d-6           [-1, 32, 53, 53]               0\n","#             Linear-7                  [-1, 512]      46,023,168\n","#             Linear-8                    [-1, 2]           1,026\n","#            Softmax-9                    [-1, 2]               0\n","# ================================================================\n","# Total params: 46,038,242\n","# Trainable params: 46,038,242\n","# Non-trainable params: 0\n","# ----------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"aZaupY3r5dCq"},"source":["#### 2. AlexNet\n","\n","- LeNet-5와 구조에서는 유사\n","\n","- 2개의 GPU에 병렬로 연산 (코드에 어떻게 구현되어 있는지 봐보기)\n","\n","- 책 상의 GPU의 결과 확인해보기 (필터로 추정됨)"]},{"cell_type":"markdown","metadata":{"id":"eUWK518dPDVX"},"source":["##### AlexNet architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qX24rjpO4r73"},"outputs":[],"source":["# ----------------------------------------------------------------\n","#         Layer (type)               Output Shape         Param #\n","# ================================================================\n","#             Conv2d-1           [-1, 64, 63, 63]          23,296\n","#               ReLU-2           [-1, 64, 63, 63]               0\n","#          MaxPool2d-3           [-1, 64, 31, 31]               0\n","#             Conv2d-4          [-1, 192, 31, 31]         307,392\n","#               ReLU-5          [-1, 192, 31, 31]               0\n","#          MaxPool2d-6          [-1, 192, 15, 15]               0\n","#             Conv2d-7          [-1, 384, 15, 15]         663,936\n","#               ReLU-8          [-1, 384, 15, 15]               0\n","#             Conv2d-9          [-1, 256, 15, 15]         884,992\n","#              ReLU-10          [-1, 256, 15, 15]               0\n","#            Conv2d-11          [-1, 256, 15, 15]         590,080\n","#              ReLU-12          [-1, 256, 15, 15]               0\n","#         MaxPool2d-13            [-1, 256, 7, 7]               0\n","# AdaptiveAvgPool2d-14            [-1, 256, 6, 6]               0\n","#           Dropout-15                 [-1, 9216]               0\n","#            Linear-16                 [-1, 4096]      37,752,832\n","#              ReLU-17                 [-1, 4096]               0\n","#           Dropout-18                 [-1, 4096]               0\n","#            Linear-19                  [-1, 512]       2,097,664\n","#              ReLU-20                  [-1, 512]               0\n","#            Linear-21                    [-1, 2]           1,026\n","# ================================================================\n","# Total params: 42,321,218\n","# Trainable params: 42,321,218\n","# Non-trainable params: 0\n","# ----------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"WUs_h_kW4wZf"},"source":["#### 3. VGGNet\n","\n","- 깊이가 딥러닝 모델에 미치는 성능을 확인하기 위해서 개발 (Very deep convolutional networks for large-scal image recognition)\n","\n","- Conv의 kernel size: 3x3, MaxPool의 kernel size: 3x3이고 stride: 2로 고정하였으며, 모든 활성화 함수는 ReLU"]},{"cell_type":"markdown","metadata":{"id":"oZN0b-uzPMco"},"source":["##### VGGNet architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bj6WvFpeYJC5"},"outputs":[],"source":["# ----------------------------------------------------------------\n","#         Layer (type)               Output Shape         Param #\n","# ================================================================\n","#             Conv2d-1         [-1, 64, 256, 256]           1,792\n","#        BatchNorm2d-2         [-1, 64, 256, 256]             128\n","#               ReLU-3         [-1, 64, 256, 256]               0\n","#          MaxPool2d-4         [-1, 64, 128, 128]               0\n","#             Conv2d-5        [-1, 128, 128, 128]          73,856\n","#        BatchNorm2d-6        [-1, 128, 128, 128]             256\n","#               ReLU-7        [-1, 128, 128, 128]               0\n","#          MaxPool2d-8          [-1, 128, 64, 64]               0\n","#             Conv2d-9          [-1, 256, 64, 64]         295,168\n","#       BatchNorm2d-10          [-1, 256, 64, 64]             512\n","#              ReLU-11          [-1, 256, 64, 64]               0\n","#            Conv2d-12          [-1, 256, 64, 64]         590,080\n","#       BatchNorm2d-13          [-1, 256, 64, 64]             512\n","#              ReLU-14          [-1, 256, 64, 64]               0\n","#         MaxPool2d-15          [-1, 256, 32, 32]               0\n","#            Conv2d-16          [-1, 512, 32, 32]       1,180,160\n","#       BatchNorm2d-17          [-1, 512, 32, 32]           1,024\n","#              ReLU-18          [-1, 512, 32, 32]               0\n","#            Conv2d-19          [-1, 512, 32, 32]       2,359,808\n","#       BatchNorm2d-20          [-1, 512, 32, 32]           1,024\n","#              ReLU-21          [-1, 512, 32, 32]               0\n","#         MaxPool2d-22          [-1, 512, 16, 16]               0\n","#            Conv2d-23          [-1, 512, 16, 16]       2,359,808\n","#       BatchNorm2d-24          [-1, 512, 16, 16]           1,024\n","#              ReLU-25          [-1, 512, 16, 16]               0\n","#            Conv2d-26          [-1, 512, 16, 16]       2,359,808\n","#       BatchNorm2d-27          [-1, 512, 16, 16]           1,024\n","#              ReLU-28          [-1, 512, 16, 16]               0\n","#         MaxPool2d-29            [-1, 512, 8, 8]               0\n","# ================================================================\n","# Total params: 9,225,984\n","# Trainable params: 9,225,984\n","# Non-trainable params: 0\n","# ----------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"sjFAqzgjOHTp"},"source":["#### 4. ResNet\n","\n","- Residual block을 도입하였으며, residual block에서는 shortcut(skip connection)을 이용하여서 네트워크의 깊이가 깊어지더라도 성능이 저하되는 것을 방지\n","\n","- Identity mapping(shortcut, skip connection)은 입력 x가 함수를 통과하였을때, 다시 x가 출력되는 것을 의미"]}],"metadata":{"colab":{"collapsed_sections":["5fV7A3thOlJ4","eUWK518dPDVX","oZN0b-uzPMco"],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.7 ('tf26')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1e329751ad3fbc78699a5e88abe00d39aafe605dca43acf9894240e004a03697"}}},"nbformat":4,"nbformat_minor":0}
